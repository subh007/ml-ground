{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhGVic6B7APtBJ6mJrl07W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subh007/ml-ground/blob/main/pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pytorch exploration"
      ],
      "metadata": {
        "id": "KHnjyRiGB3Bu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6J19zNmB2T-",
        "outputId": "709c6113-f245-4f99-ab64-20791a11d440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch  matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "j13RDhDnCOJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1,1]\n",
        "# ])"
      ],
      "metadata": {
        "id": "kLLmSZjkCnoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "# test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASEO5iTcEbEO",
        "outputId": "013b9a5d-1280-4cb1-da68-fa9c8746905a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 38.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.18MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.05MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# implement simle perceptron using pytorch\n"
      ],
      "metadata": {
        "id": "eo-8iWk3t1uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "DzX4TMUkthHt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare XOR data\n",
        "x = torch.tensor([[0,0], [0,1], [1,0], [1, 1]], dtype=torch.float32)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "rqbGdEHSxHOV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define\n",
        "class Perceptron(nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "    super(Perceptron, self).__init__()\n",
        "    self.fc = nn.Linear(input_size, 1)     # (input_feature, out_feature)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.sigmoid(self.fc(x))\n",
        "    return out"
      ],
      "metadata": {
        "id": "_-pxU65xx548"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model loss and optimizer\n",
        "# model = Perceptron(2)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)      #optimizer"
      ],
      "metadata": {
        "id": "DiFvEW9Tx-2N"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# execute the model for 100 times\n",
        "for i in range(500):\n",
        "  optimizer.zero_grad()  # reset gradient\n",
        "  output = model(x)\n",
        "  loss = criterion(output, y)\n",
        "  loss.backward()       # backpropagation\n",
        "  optimizer.step()\n",
        "  print(f'Epoch {i}, Loss:{loss}')"
      ],
      "metadata": {
        "id": "XHMxb0Y_zERn",
        "outputId": "26aefd79-de5c-4503-fdcd-57c9926a18fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss:0.7676912546157837\n",
            "Epoch 1, Loss:0.767163872718811\n",
            "Epoch 2, Loss:0.766639769077301\n",
            "Epoch 3, Loss:0.7661193013191223\n",
            "Epoch 4, Loss:0.7656022310256958\n",
            "Epoch 5, Loss:0.7650887370109558\n",
            "Epoch 6, Loss:0.7645785212516785\n",
            "Epoch 7, Loss:0.7640718817710876\n",
            "Epoch 8, Loss:0.7635685205459595\n",
            "Epoch 9, Loss:0.7630685567855835\n",
            "Epoch 10, Loss:0.7625719308853149\n",
            "Epoch 11, Loss:0.7620787024497986\n",
            "Epoch 12, Loss:0.7615888118743896\n",
            "Epoch 13, Loss:0.7611021995544434\n",
            "Epoch 14, Loss:0.7606188058853149\n",
            "Epoch 15, Loss:0.7601388096809387\n",
            "Epoch 16, Loss:0.7596619129180908\n",
            "Epoch 17, Loss:0.7591882944107056\n",
            "Epoch 18, Loss:0.7587180137634277\n",
            "Epoch 19, Loss:0.7582507729530334\n",
            "Epoch 20, Loss:0.757786750793457\n",
            "Epoch 21, Loss:0.7573258876800537\n",
            "Epoch 22, Loss:0.7568681240081787\n",
            "Epoch 23, Loss:0.7564135789871216\n",
            "Epoch 24, Loss:0.7559620141983032\n",
            "Epoch 25, Loss:0.755513608455658\n",
            "Epoch 26, Loss:0.7550682425498962\n",
            "Epoch 27, Loss:0.7546259760856628\n",
            "Epoch 28, Loss:0.7541866898536682\n",
            "Epoch 29, Loss:0.7537504434585571\n",
            "Epoch 30, Loss:0.75331711769104\n",
            "Epoch 31, Loss:0.7528868317604065\n",
            "Epoch 32, Loss:0.7524594664573669\n",
            "Epoch 33, Loss:0.7520350813865662\n",
            "Epoch 34, Loss:0.7516136169433594\n",
            "Epoch 35, Loss:0.7511950135231018\n",
            "Epoch 36, Loss:0.750779390335083\n",
            "Epoch 37, Loss:0.7503666281700134\n",
            "Epoch 38, Loss:0.7499566674232483\n",
            "Epoch 39, Loss:0.7495496273040771\n",
            "Epoch 40, Loss:0.7491451501846313\n",
            "Epoch 41, Loss:0.7487437725067139\n",
            "Epoch 42, Loss:0.7483450770378113\n",
            "Epoch 43, Loss:0.7479491233825684\n",
            "Epoch 44, Loss:0.7475559711456299\n",
            "Epoch 45, Loss:0.7471655011177063\n",
            "Epoch 46, Loss:0.7467778325080872\n",
            "Epoch 47, Loss:0.7463927268981934\n",
            "Epoch 48, Loss:0.746010422706604\n",
            "Epoch 49, Loss:0.7456308603286743\n",
            "Epoch 50, Loss:0.7452538013458252\n",
            "Epoch 51, Loss:0.7448794841766357\n",
            "Epoch 52, Loss:0.7445077300071716\n",
            "Epoch 53, Loss:0.7441385984420776\n",
            "Epoch 54, Loss:0.7437720894813538\n",
            "Epoch 55, Loss:0.7434080839157104\n",
            "Epoch 56, Loss:0.7430466413497925\n",
            "Epoch 57, Loss:0.7426878213882446\n",
            "Epoch 58, Loss:0.7423313856124878\n",
            "Epoch 59, Loss:0.7419775724411011\n",
            "Epoch 60, Loss:0.7416262030601501\n",
            "Epoch 61, Loss:0.7412773966789246\n",
            "Epoch 62, Loss:0.74093097448349\n",
            "Epoch 63, Loss:0.7405869960784912\n",
            "Epoch 64, Loss:0.7402454614639282\n",
            "Epoch 65, Loss:0.739906370639801\n",
            "Epoch 66, Loss:0.7395697236061096\n",
            "Epoch 67, Loss:0.7392354011535645\n",
            "Epoch 68, Loss:0.7389034628868103\n",
            "Epoch 69, Loss:0.7385737895965576\n",
            "Epoch 70, Loss:0.7382465600967407\n",
            "Epoch 71, Loss:0.7379217147827148\n",
            "Epoch 72, Loss:0.7375990152359009\n",
            "Epoch 73, Loss:0.7372786998748779\n",
            "Epoch 74, Loss:0.7369607090950012\n",
            "Epoch 75, Loss:0.7366449236869812\n",
            "Epoch 76, Loss:0.7363314628601074\n",
            "Epoch 77, Loss:0.7360201478004456\n",
            "Epoch 78, Loss:0.7357110977172852\n",
            "Epoch 79, Loss:0.7354042530059814\n",
            "Epoch 80, Loss:0.7350995540618896\n",
            "Epoch 81, Loss:0.7347971796989441\n",
            "Epoch 82, Loss:0.7344968318939209\n",
            "Epoch 83, Loss:0.7341986894607544\n",
            "Epoch 84, Loss:0.733902633190155\n",
            "Epoch 85, Loss:0.7336087822914124\n",
            "Epoch 86, Loss:0.7333170175552368\n",
            "Epoch 87, Loss:0.7330273985862732\n",
            "Epoch 88, Loss:0.7327396869659424\n",
            "Epoch 89, Loss:0.732454240322113\n",
            "Epoch 90, Loss:0.732170820236206\n",
            "Epoch 91, Loss:0.7318893074989319\n",
            "Epoch 92, Loss:0.7316099405288696\n",
            "Epoch 93, Loss:0.731332540512085\n",
            "Epoch 94, Loss:0.7310571670532227\n",
            "Epoch 95, Loss:0.7307838201522827\n",
            "Epoch 96, Loss:0.7305123805999756\n",
            "Epoch 97, Loss:0.7302428483963013\n",
            "Epoch 98, Loss:0.7299753427505493\n",
            "Epoch 99, Loss:0.7297098636627197\n",
            "Epoch 100, Loss:0.7294461727142334\n",
            "Epoch 101, Loss:0.7291843891143799\n",
            "Epoch 102, Loss:0.728924572467804\n",
            "Epoch 103, Loss:0.7286666631698608\n",
            "Epoch 104, Loss:0.7284106016159058\n",
            "Epoch 105, Loss:0.728156328201294\n",
            "Epoch 106, Loss:0.7279039621353149\n",
            "Epoch 107, Loss:0.727653443813324\n",
            "Epoch 108, Loss:0.7274047136306763\n",
            "Epoch 109, Loss:0.727157711982727\n",
            "Epoch 110, Loss:0.7269126772880554\n",
            "Epoch 111, Loss:0.726669430732727\n",
            "Epoch 112, Loss:0.7264278531074524\n",
            "Epoch 113, Loss:0.726188063621521\n",
            "Epoch 114, Loss:0.7259500026702881\n",
            "Epoch 115, Loss:0.7257137298583984\n",
            "Epoch 116, Loss:0.7254791259765625\n",
            "Epoch 117, Loss:0.725246250629425\n",
            "Epoch 118, Loss:0.7250151634216309\n",
            "Epoch 119, Loss:0.7247856259346008\n",
            "Epoch 120, Loss:0.7245578765869141\n",
            "Epoch 121, Loss:0.7243317365646362\n",
            "Epoch 122, Loss:0.7241073250770569\n",
            "Epoch 123, Loss:0.7238844633102417\n",
            "Epoch 124, Loss:0.7236632704734802\n",
            "Epoch 125, Loss:0.7234437465667725\n",
            "Epoch 126, Loss:0.7232258319854736\n",
            "Epoch 127, Loss:0.7230094075202942\n",
            "Epoch 128, Loss:0.7227947115898132\n",
            "Epoch 129, Loss:0.7225815653800964\n",
            "Epoch 130, Loss:0.722369909286499\n",
            "Epoch 131, Loss:0.7221598625183105\n",
            "Epoch 132, Loss:0.7219513058662415\n",
            "Epoch 133, Loss:0.7217443585395813\n",
            "Epoch 134, Loss:0.7215389013290405\n",
            "Epoch 135, Loss:0.7213350534439087\n",
            "Epoch 136, Loss:0.7211325168609619\n",
            "Epoch 137, Loss:0.7209315896034241\n",
            "Epoch 138, Loss:0.7207321524620056\n",
            "Epoch 139, Loss:0.7205342054367065\n",
            "Epoch 140, Loss:0.7203376293182373\n",
            "Epoch 141, Loss:0.7201425433158875\n",
            "Epoch 142, Loss:0.7199488878250122\n",
            "Epoch 143, Loss:0.7197567224502563\n",
            "Epoch 144, Loss:0.7195659279823303\n",
            "Epoch 145, Loss:0.7193766236305237\n",
            "Epoch 146, Loss:0.7191885709762573\n",
            "Epoch 147, Loss:0.7190020084381104\n",
            "Epoch 148, Loss:0.7188168168067932\n",
            "Epoch 149, Loss:0.7186330556869507\n",
            "Epoch 150, Loss:0.7184506058692932\n",
            "Epoch 151, Loss:0.7182694673538208\n",
            "Epoch 152, Loss:0.718089759349823\n",
            "Epoch 153, Loss:0.7179112434387207\n",
            "Epoch 154, Loss:0.7177342176437378\n",
            "Epoch 155, Loss:0.7175584435462952\n",
            "Epoch 156, Loss:0.717383861541748\n",
            "Epoch 157, Loss:0.7172107100486755\n",
            "Epoch 158, Loss:0.7170388698577881\n",
            "Epoch 159, Loss:0.7168681025505066\n",
            "Epoch 160, Loss:0.7166988253593445\n",
            "Epoch 161, Loss:0.7165306806564331\n",
            "Epoch 162, Loss:0.7163638472557068\n",
            "Epoch 163, Loss:0.7161982655525208\n",
            "Epoch 164, Loss:0.7160338759422302\n",
            "Epoch 165, Loss:0.71587073802948\n",
            "Epoch 166, Loss:0.7157087922096252\n",
            "Epoch 167, Loss:0.7155480980873108\n",
            "Epoch 168, Loss:0.7153885960578918\n",
            "Epoch 169, Loss:0.7152302265167236\n",
            "Epoch 170, Loss:0.7150730490684509\n",
            "Epoch 171, Loss:0.7149171829223633\n",
            "Epoch 172, Loss:0.7147622108459473\n",
            "Epoch 173, Loss:0.7146086096763611\n",
            "Epoch 174, Loss:0.7144560813903809\n",
            "Epoch 175, Loss:0.7143047451972961\n",
            "Epoch 176, Loss:0.7141544818878174\n",
            "Epoch 177, Loss:0.7140053510665894\n",
            "Epoch 178, Loss:0.7138573527336121\n",
            "Epoch 179, Loss:0.7137104272842407\n",
            "Epoch 180, Loss:0.7135646343231201\n",
            "Epoch 181, Loss:0.7134198546409607\n",
            "Epoch 182, Loss:0.7132762670516968\n",
            "Epoch 183, Loss:0.713133692741394\n",
            "Epoch 184, Loss:0.712992250919342\n",
            "Epoch 185, Loss:0.712851881980896\n",
            "Epoch 186, Loss:0.7127125263214111\n",
            "Epoch 187, Loss:0.7125740647315979\n",
            "Epoch 188, Loss:0.7124367952346802\n",
            "Epoch 189, Loss:0.7123005390167236\n",
            "Epoch 190, Loss:0.7121652364730835\n",
            "Epoch 191, Loss:0.7120310068130493\n",
            "Epoch 192, Loss:0.7118977904319763\n",
            "Epoch 193, Loss:0.7117656469345093\n",
            "Epoch 194, Loss:0.7116343379020691\n",
            "Epoch 195, Loss:0.7115041017532349\n",
            "Epoch 196, Loss:0.7113748788833618\n",
            "Epoch 197, Loss:0.7112465500831604\n",
            "Epoch 198, Loss:0.7111191749572754\n",
            "Epoch 199, Loss:0.7109928727149963\n",
            "Epoch 200, Loss:0.7108674049377441\n",
            "Epoch 201, Loss:0.7107428312301636\n",
            "Epoch 202, Loss:0.710619330406189\n",
            "Epoch 203, Loss:0.7104966640472412\n",
            "Epoch 204, Loss:0.7103749513626099\n",
            "Epoch 205, Loss:0.7102541923522949\n",
            "Epoch 206, Loss:0.7101342678070068\n",
            "Epoch 207, Loss:0.7100152969360352\n",
            "Epoch 208, Loss:0.7098972797393799\n",
            "Epoch 209, Loss:0.7097800970077515\n",
            "Epoch 210, Loss:0.7096637487411499\n",
            "Epoch 211, Loss:0.70954829454422\n",
            "Epoch 212, Loss:0.7094337940216064\n",
            "Epoch 213, Loss:0.709320068359375\n",
            "Epoch 214, Loss:0.7092071771621704\n",
            "Epoch 215, Loss:0.7090952396392822\n",
            "Epoch 216, Loss:0.7089840769767761\n",
            "Epoch 217, Loss:0.7088736891746521\n",
            "Epoch 218, Loss:0.7087641954421997\n",
            "Epoch 219, Loss:0.7086555361747742\n",
            "Epoch 220, Loss:0.7085477113723755\n",
            "Epoch 221, Loss:0.7084406614303589\n",
            "Epoch 222, Loss:0.7083344459533691\n",
            "Epoch 223, Loss:0.7082290649414062\n",
            "Epoch 224, Loss:0.7081243991851807\n",
            "Epoch 225, Loss:0.7080205678939819\n",
            "Epoch 226, Loss:0.7079175114631653\n",
            "Epoch 227, Loss:0.7078152894973755\n",
            "Epoch 228, Loss:0.7077137231826782\n",
            "Epoch 229, Loss:0.7076129913330078\n",
            "Epoch 230, Loss:0.7075129747390747\n",
            "Epoch 231, Loss:0.7074137926101685\n",
            "Epoch 232, Loss:0.7073152661323547\n",
            "Epoch 233, Loss:0.7072175145149231\n",
            "Epoch 234, Loss:0.7071205377578735\n",
            "Epoch 235, Loss:0.7070242762565613\n",
            "Epoch 236, Loss:0.7069287300109863\n",
            "Epoch 237, Loss:0.7068339586257935\n",
            "Epoch 238, Loss:0.7067397832870483\n",
            "Epoch 239, Loss:0.7066463828086853\n",
            "Epoch 240, Loss:0.7065537571907043\n",
            "Epoch 241, Loss:0.7064617872238159\n",
            "Epoch 242, Loss:0.70637047290802\n",
            "Epoch 243, Loss:0.7062798738479614\n",
            "Epoch 244, Loss:0.7061899900436401\n",
            "Epoch 245, Loss:0.7061007022857666\n",
            "Epoch 246, Loss:0.7060120701789856\n",
            "Epoch 247, Loss:0.7059241533279419\n",
            "Epoch 248, Loss:0.7058370113372803\n",
            "Epoch 249, Loss:0.7057503461837769\n",
            "Epoch 250, Loss:0.7056644558906555\n",
            "Epoch 251, Loss:0.7055792212486267\n",
            "Epoch 252, Loss:0.7054945826530457\n",
            "Epoch 253, Loss:0.7054106593132019\n",
            "Epoch 254, Loss:0.7053272128105164\n",
            "Epoch 255, Loss:0.7052446007728577\n",
            "Epoch 256, Loss:0.7051624059677124\n",
            "Epoch 257, Loss:0.7050809264183044\n",
            "Epoch 258, Loss:0.705000102519989\n",
            "Epoch 259, Loss:0.7049198746681213\n",
            "Epoch 260, Loss:0.7048403024673462\n",
            "Epoch 261, Loss:0.7047611474990845\n",
            "Epoch 262, Loss:0.7046827673912048\n",
            "Epoch 263, Loss:0.7046048641204834\n",
            "Epoch 264, Loss:0.7045275568962097\n",
            "Epoch 265, Loss:0.7044510245323181\n",
            "Epoch 266, Loss:0.7043748497962952\n",
            "Epoch 267, Loss:0.7042993307113647\n",
            "Epoch 268, Loss:0.7042243480682373\n",
            "Epoch 269, Loss:0.7041499614715576\n",
            "Epoch 270, Loss:0.7040762305259705\n",
            "Epoch 271, Loss:0.7040029764175415\n",
            "Epoch 272, Loss:0.7039302587509155\n",
            "Epoch 273, Loss:0.7038580179214478\n",
            "Epoch 274, Loss:0.7037864327430725\n",
            "Epoch 275, Loss:0.7037153840065002\n",
            "Epoch 276, Loss:0.7036448121070862\n",
            "Epoch 277, Loss:0.7035747766494751\n",
            "Epoch 278, Loss:0.7035053968429565\n",
            "Epoch 279, Loss:0.7034363746643066\n",
            "Epoch 280, Loss:0.7033680081367493\n",
            "Epoch 281, Loss:0.7033001184463501\n",
            "Epoch 282, Loss:0.7032326459884644\n",
            "Epoch 283, Loss:0.7031657695770264\n",
            "Epoch 284, Loss:0.7030994296073914\n",
            "Epoch 285, Loss:0.7030335664749146\n",
            "Epoch 286, Loss:0.702968180179596\n",
            "Epoch 287, Loss:0.7029032707214355\n",
            "Epoch 288, Loss:0.7028388977050781\n",
            "Epoch 289, Loss:0.7027750015258789\n",
            "Epoch 290, Loss:0.7027115821838379\n",
            "Epoch 291, Loss:0.7026486396789551\n",
            "Epoch 292, Loss:0.7025861740112305\n",
            "Epoch 293, Loss:0.7025241255760193\n",
            "Epoch 294, Loss:0.7024626135826111\n",
            "Epoch 295, Loss:0.7024015188217163\n",
            "Epoch 296, Loss:0.7023409008979797\n",
            "Epoch 297, Loss:0.7022808194160461\n",
            "Epoch 298, Loss:0.7022210955619812\n",
            "Epoch 299, Loss:0.7021619081497192\n",
            "Epoch 300, Loss:0.7021030783653259\n",
            "Epoch 301, Loss:0.7020447254180908\n",
            "Epoch 302, Loss:0.7019867897033691\n",
            "Epoch 303, Loss:0.7019293308258057\n",
            "Epoch 304, Loss:0.7018723487854004\n",
            "Epoch 305, Loss:0.7018157243728638\n",
            "Epoch 306, Loss:0.7017595767974854\n",
            "Epoch 307, Loss:0.7017037868499756\n",
            "Epoch 308, Loss:0.7016484141349792\n",
            "Epoch 309, Loss:0.7015935182571411\n",
            "Epoch 310, Loss:0.7015390396118164\n",
            "Epoch 311, Loss:0.7014849185943604\n",
            "Epoch 312, Loss:0.7014312744140625\n",
            "Epoch 313, Loss:0.7013779878616333\n",
            "Epoch 314, Loss:0.7013251185417175\n",
            "Epoch 315, Loss:0.7012726664543152\n",
            "Epoch 316, Loss:0.7012205719947815\n",
            "Epoch 317, Loss:0.7011688947677612\n",
            "Epoch 318, Loss:0.7011175751686096\n",
            "Epoch 319, Loss:0.7010666131973267\n",
            "Epoch 320, Loss:0.7010161876678467\n",
            "Epoch 321, Loss:0.7009660601615906\n",
            "Epoch 322, Loss:0.7009162306785583\n",
            "Epoch 323, Loss:0.7008668184280396\n",
            "Epoch 324, Loss:0.7008178234100342\n",
            "Epoch 325, Loss:0.7007691860198975\n",
            "Epoch 326, Loss:0.7007208466529846\n",
            "Epoch 327, Loss:0.7006729245185852\n",
            "Epoch 328, Loss:0.7006253600120544\n",
            "Epoch 329, Loss:0.7005782127380371\n",
            "Epoch 330, Loss:0.7005313634872437\n",
            "Epoch 331, Loss:0.7004848122596741\n",
            "Epoch 332, Loss:0.7004386782646179\n",
            "Epoch 333, Loss:0.7003929018974304\n",
            "Epoch 334, Loss:0.7003475427627563\n",
            "Epoch 335, Loss:0.7003024220466614\n",
            "Epoch 336, Loss:0.7002575397491455\n",
            "Epoch 337, Loss:0.7002131938934326\n",
            "Epoch 338, Loss:0.7001691460609436\n",
            "Epoch 339, Loss:0.7001252770423889\n",
            "Epoch 340, Loss:0.7000818848609924\n",
            "Epoch 341, Loss:0.700038731098175\n",
            "Epoch 342, Loss:0.6999960541725159\n",
            "Epoch 343, Loss:0.699953556060791\n",
            "Epoch 344, Loss:0.69991135597229\n",
            "Epoch 345, Loss:0.6998695135116577\n",
            "Epoch 346, Loss:0.699828028678894\n",
            "Epoch 347, Loss:0.6997868418693542\n",
            "Epoch 348, Loss:0.6997458934783936\n",
            "Epoch 349, Loss:0.6997053623199463\n",
            "Epoch 350, Loss:0.6996650695800781\n",
            "Epoch 351, Loss:0.6996250748634338\n",
            "Epoch 352, Loss:0.6995853781700134\n",
            "Epoch 353, Loss:0.6995460391044617\n",
            "Epoch 354, Loss:0.699506938457489\n",
            "Epoch 355, Loss:0.6994681358337402\n",
            "Epoch 356, Loss:0.6994295716285706\n",
            "Epoch 357, Loss:0.6993913650512695\n",
            "Epoch 358, Loss:0.6993534564971924\n",
            "Epoch 359, Loss:0.6993157863616943\n",
            "Epoch 360, Loss:0.6992785334587097\n",
            "Epoch 361, Loss:0.6992413401603699\n",
            "Epoch 362, Loss:0.6992046236991882\n",
            "Epoch 363, Loss:0.6991680860519409\n",
            "Epoch 364, Loss:0.6991318464279175\n",
            "Epoch 365, Loss:0.6990957856178284\n",
            "Epoch 366, Loss:0.6990601420402527\n",
            "Epoch 367, Loss:0.6990246176719666\n",
            "Epoch 368, Loss:0.6989895105361938\n",
            "Epoch 369, Loss:0.6989545822143555\n",
            "Epoch 370, Loss:0.6989198923110962\n",
            "Epoch 371, Loss:0.6988855004310608\n",
            "Epoch 372, Loss:0.6988512873649597\n",
            "Epoch 373, Loss:0.6988174319267273\n",
            "Epoch 374, Loss:0.6987838745117188\n",
            "Epoch 375, Loss:0.698750376701355\n",
            "Epoch 376, Loss:0.6987172365188599\n",
            "Epoch 377, Loss:0.6986844539642334\n",
            "Epoch 378, Loss:0.6986517906188965\n",
            "Epoch 379, Loss:0.6986194252967834\n",
            "Epoch 380, Loss:0.6985872387886047\n",
            "Epoch 381, Loss:0.6985552906990051\n",
            "Epoch 382, Loss:0.6985236406326294\n",
            "Epoch 383, Loss:0.6984922289848328\n",
            "Epoch 384, Loss:0.6984609365463257\n",
            "Epoch 385, Loss:0.698430061340332\n",
            "Epoch 386, Loss:0.6983991861343384\n",
            "Epoch 387, Loss:0.6983687877655029\n",
            "Epoch 388, Loss:0.6983384490013123\n",
            "Epoch 389, Loss:0.6983084678649902\n",
            "Epoch 390, Loss:0.698278546333313\n",
            "Epoch 391, Loss:0.6982489824295044\n",
            "Epoch 392, Loss:0.6982195377349854\n",
            "Epoch 393, Loss:0.6981903910636902\n",
            "Epoch 394, Loss:0.6981613636016846\n",
            "Epoch 395, Loss:0.6981325745582581\n",
            "Epoch 396, Loss:0.6981041431427002\n",
            "Epoch 397, Loss:0.6980757713317871\n",
            "Epoch 398, Loss:0.6980476975440979\n",
            "Epoch 399, Loss:0.6980197429656982\n",
            "Epoch 400, Loss:0.6979920864105225\n",
            "Epoch 401, Loss:0.6979645490646362\n",
            "Epoch 402, Loss:0.6979372501373291\n",
            "Epoch 403, Loss:0.6979102492332458\n",
            "Epoch 404, Loss:0.6978833675384521\n",
            "Epoch 405, Loss:0.6978565454483032\n",
            "Epoch 406, Loss:0.6978301405906677\n",
            "Epoch 407, Loss:0.6978038549423218\n",
            "Epoch 408, Loss:0.6977777481079102\n",
            "Epoch 409, Loss:0.6977517604827881\n",
            "Epoch 410, Loss:0.6977260708808899\n",
            "Epoch 411, Loss:0.697700560092926\n",
            "Epoch 412, Loss:0.6976752281188965\n",
            "Epoch 413, Loss:0.6976500749588013\n",
            "Epoch 414, Loss:0.6976251602172852\n",
            "Epoch 415, Loss:0.6976003646850586\n",
            "Epoch 416, Loss:0.6975756883621216\n",
            "Epoch 417, Loss:0.6975513100624084\n",
            "Epoch 418, Loss:0.6975271105766296\n",
            "Epoch 419, Loss:0.6975030899047852\n",
            "Epoch 420, Loss:0.6974791288375854\n",
            "Epoch 421, Loss:0.6974554061889648\n",
            "Epoch 422, Loss:0.6974319219589233\n",
            "Epoch 423, Loss:0.6974084973335266\n",
            "Epoch 424, Loss:0.697385311126709\n",
            "Epoch 425, Loss:0.6973624229431152\n",
            "Epoch 426, Loss:0.6973395347595215\n",
            "Epoch 427, Loss:0.6973167657852173\n",
            "Epoch 428, Loss:0.6972942352294922\n",
            "Epoch 429, Loss:0.6972719430923462\n",
            "Epoch 430, Loss:0.697249710559845\n",
            "Epoch 431, Loss:0.6972277164459229\n",
            "Epoch 432, Loss:0.6972057819366455\n",
            "Epoch 433, Loss:0.6971842050552368\n",
            "Epoch 434, Loss:0.6971626281738281\n",
            "Epoch 435, Loss:0.6971412301063538\n",
            "Epoch 436, Loss:0.697119951248169\n",
            "Epoch 437, Loss:0.6970989108085632\n",
            "Epoch 438, Loss:0.6970780491828918\n",
            "Epoch 439, Loss:0.6970572471618652\n",
            "Epoch 440, Loss:0.697036623954773\n",
            "Epoch 441, Loss:0.697016179561615\n",
            "Epoch 442, Loss:0.6969958543777466\n",
            "Epoch 443, Loss:0.6969757676124573\n",
            "Epoch 444, Loss:0.696955680847168\n",
            "Epoch 445, Loss:0.696935772895813\n",
            "Epoch 446, Loss:0.6969161033630371\n",
            "Epoch 447, Loss:0.696896493434906\n",
            "Epoch 448, Loss:0.6968770027160645\n",
            "Epoch 449, Loss:0.696857750415802\n",
            "Epoch 450, Loss:0.6968386173248291\n",
            "Epoch 451, Loss:0.6968196034431458\n",
            "Epoch 452, Loss:0.6968006491661072\n",
            "Epoch 453, Loss:0.6967819333076477\n",
            "Epoch 454, Loss:0.6967633366584778\n",
            "Epoch 455, Loss:0.6967447996139526\n",
            "Epoch 456, Loss:0.6967264413833618\n",
            "Epoch 457, Loss:0.6967082619667053\n",
            "Epoch 458, Loss:0.6966902017593384\n",
            "Epoch 459, Loss:0.6966722011566162\n",
            "Epoch 460, Loss:0.6966543793678284\n",
            "Epoch 461, Loss:0.6966367959976196\n",
            "Epoch 462, Loss:0.6966192126274109\n",
            "Epoch 463, Loss:0.6966017484664917\n",
            "Epoch 464, Loss:0.6965844035148621\n",
            "Epoch 465, Loss:0.6965672373771667\n",
            "Epoch 466, Loss:0.696550190448761\n",
            "Epoch 467, Loss:0.6965332627296448\n",
            "Epoch 468, Loss:0.6965164542198181\n",
            "Epoch 469, Loss:0.6964997053146362\n",
            "Epoch 470, Loss:0.6964832544326782\n",
            "Epoch 471, Loss:0.6964666843414307\n",
            "Epoch 472, Loss:0.6964503526687622\n",
            "Epoch 473, Loss:0.6964341998100281\n",
            "Epoch 474, Loss:0.6964181065559387\n",
            "Epoch 475, Loss:0.6964020729064941\n",
            "Epoch 476, Loss:0.6963862180709839\n",
            "Epoch 477, Loss:0.6963704824447632\n",
            "Epoch 478, Loss:0.6963547468185425\n",
            "Epoch 479, Loss:0.6963393092155457\n",
            "Epoch 480, Loss:0.6963238716125488\n",
            "Epoch 481, Loss:0.6963084936141968\n",
            "Epoch 482, Loss:0.696293294429779\n",
            "Epoch 483, Loss:0.6962782144546509\n",
            "Epoch 484, Loss:0.6962631940841675\n",
            "Epoch 485, Loss:0.6962483525276184\n",
            "Epoch 486, Loss:0.6962335109710693\n",
            "Epoch 487, Loss:0.6962189674377441\n",
            "Epoch 488, Loss:0.6962043642997742\n",
            "Epoch 489, Loss:0.6961898803710938\n",
            "Epoch 490, Loss:0.6961755752563477\n",
            "Epoch 491, Loss:0.6961612701416016\n",
            "Epoch 492, Loss:0.696147084236145\n",
            "Epoch 493, Loss:0.6961330771446228\n",
            "Epoch 494, Loss:0.6961190700531006\n",
            "Epoch 495, Loss:0.6961052417755127\n",
            "Epoch 496, Loss:0.6960914134979248\n",
            "Epoch 497, Loss:0.696077823638916\n",
            "Epoch 498, Loss:0.6960643529891968\n",
            "Epoch 499, Loss:0.696050763130188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "with torch.no_grad():\n",
        "  predicted = model(x)\n",
        "  predicted = (predicted > 0.5).float()\n",
        "  print(predicted)"
      ],
      "metadata": {
        "id": "irIoBa1n0_eq",
        "outputId": "efd97a23-58c4-47d8-f3c8-6ce848a568b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'perceptron.pth')"
      ],
      "metadata": {
        "id": "2Bgrzlk83vdx"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}